\section{Introduction}

% What is the problem?
File system metadata services in HPC have scalability problems because
administrative tasks, like checkpointing~\cite{bent_plfs_2009} or scanning the
file system~\cite{zheng:pdsw2014-batchfs}, contend for the same directories and
inodes. Applications perform better with dedicated metadata
servers~\cite{sevilla:sc15-mantle, ren:sc2014-indexfs} but provisioning a
metadata server for every client is unreasonable. This problem is exacerbated
by current trends in HPC, where architectures are transitioning from complex
storage stacks with burst buffer, file system, object store, and tape tiers to
more simplified stacks with just a burst buffer and object
store~\cite{bent:login16-hpc-trends}; this puts more pressure on data access
because more requests end up hitting the same layer and latencies cannot be
hidden while data migrates across tiers.

% What is HPC doing?
To address this, developers are relaxing the consistency and durability
semantics in the file system because weaker guarantees are sufficient for their
applications. For example, many batch style jobs do not need the strong
consistency that the file system provides, so
BatchFS~\cite{zheng:pdsw2014-batchfs} and DeltaFS~\cite{zheng:pdsw2015-deltafs}
do more client-side processing and merge updates when the job is done. HPC
developers are turning to these non-POSIX solutions because their applications
are well-understood ({\it e.g.}, well-defined read/write phases,
synchronization only needed during certain phases, workflows describing
computation, etc.) and because these applications wreak havoc on file systems designed for
general-purpose workloads ({\it e.g.}, checkpoint-restart's N:N and N:1 create
patterns~\cite{bent_plfs_2009}).

\begin{figure}[tb]
\centering
\includegraphics[width=0.35\textwidth]{figures/subtree-policies1.png}
\caption{ Illustration of subtrees with different semantics co-existing in a
global namespace.  For performance, clients can relax consistency on their
subtree (HDFS) or even decouple the subtree and move it locally (BatchFS,
RAMDisk). Decoupled subtrees can further relax durability for even better
performance.  Clients that require stronger guarantees (POSIX) can still reside
in the same namespace.  }\label{fig:subtree-policies}
\end{figure}

% One example
One popular approach for relaxing consistency and durability is to ``decouple
the namespace", where clients lock the subtree they want exclusive access to as
a way to tell the file system that the subtree is important or may cause
resource contention in the near-future~\cite{grider:pdsw2015-marfs,
zheng:pdsw2015-deltafs, zheng:pdsw2014-batchfs, ren:sc2014-indexfs,
bent:slides-twotiers}. Then the file system can change its internal structure
to optimize performance. For example, the file system could enter a mode that
prevents other clients from interfering with the decoupled directory.  This
delayed merge ({\it i.e.} a form of eventual consistency) and relaxed
durability improves performance and scalability by avoiding the costs of remote
procedure calls (RPCs), synchronization, false sharing, and serialization.
While the performance benefits of decoupling the namespace are obvious,
applications that rely on the file system's guarantees must be deployed on an
entirely different system or re-written to coordinate strong
consistency/durability themselves.

%\begin{table}
%\begin{tabular}{ r | l }
%  Subtree         & Example \\\hline
%  (1)   & \{Index, Batch\}FS~\cite{ren:sc2014-indexfs, zheng:pdsw2014-batchfs} \\
%  (2)   & \{Index, Ceph\}FS~\cite{ren:sc2014-indexfs, weil:sc2004-dyn-metadata} \\
%  (3)   & RAMDisk \\
%  (4)   & DeltaFS~\cite{zheng:pdsw2015-deltafs} \\
%\end{tabular}
%
%\caption{State-of-the-art systems in HPC improve file system metadata
%performance by relaxing consistency and durability guarantees. Note that
%IndexFS also supports weak consistency with bulk inserts.
%\label{table:namespaces}} \end{table}

% What did we do
To address this problem, we present an API and framework that lets developers
dynamically control the consistency and durability guarantees for subtrees in
the file system namespace.  Figure~\ref{fig:subtree-policies} shows a potential
setup in our proposed system where a single global namespace has subtrees for
applications optimized with techniques from different state-of-the-art
architectures.  The HDFS\footnote{HDFS itself is not directly evaluated in this
paper, although the HDFS semantics and their performance is explored
in~\S\ref{sec:use-case-1}} subtree has weaker consistency than strong
consistency because it does not guarantee that all updates are immediately seen
by all servers; the BatchFS and RAMDisk subtrees are decoupled from the global
namespace and have similar consistency/durability behavior to those systems;
and the POSIX subtree retains the rigidity of POSIX's strong consistency.
Subtrees without policies inherit the consistency/durability semantics of the
parent and future work will examine embeddable or inheritable policies.

Our prototype system, Cudele, achieves this by exposing ``mechanisms" that
developers use to specify their preferred semantics.  Cudele supports 3 forms
of consistency (invisible, weak, and strong) and 3 degrees of durability (none,
local, and global) giving the user a wide range of policies and optimizations
that can be custom fit to an application. We make the following contributions:

\begin{enumerate}

  \item A framework and API for assigning consistency/durability policies 
  to subtrees in the file system namespace; this lets users dynamically navigate
  the tradeoffs of different metadata protocols.

  \item This framework lets subtrees with different semantics co-exist in a
  global namespace. This is different than multiple mount points because
  there is no need to provision storage clusters for applications
  ({\it e.g.}, HDFS, CephFS) or move data between these systems.

  \item A prototype that lets developers custom fit subtrees to applications
  dynamically; this helps data center systems scale further than systems that
  use one strategy for the entire namespace.

\end{enumerate}

% Results
Our results confirm the assertions of ``clean-state" research systems that
decouple namespaces; specifically that the technique drastically improves
performance (104\(\times\) speed up) but we go a step further by quantifying
the costs of merging updates (7\(\times\) slow down) and maintaining durability
(\(10\times\) slow down). In our prototype, we get an 8\(\times\) speedup and
can scale to twice as many servers when we assign a more relaxed form of
consistency and durability to a subtree with a create heavy workload.
Meanwhile, other subtrees in the namespace maintain strong consistency and
durability with RPCs per operation.

We build on Ceph because it is used in cloud-based and data center
systems and has a presence in HPC~\cite{}.  More importantly,  we use Ceph as a
platform for exploring related work over the same storage system, which
facilitates an apples-to-apples comparison of the strategies themselves.  

In the remainder of the paper, Section~\ref{sec:posix-overheads} quantifies the
cost of POSIX consistency and system-defined durability and
Section~\ref{sec:methodology-decoupled-namespaces} presents the Cudele
prototype and API. Section~\ref{sec:implementation} describes the Cudele
mechanisms and shows how re-using internal subsystems results in an
implementation of less than 500 lines of code. The evaluation in
Section~\ref{sec:evaluation} quantifies the overheads and performance gains of
explored and previously unexplored metadata designs.
Section~\ref{sec:related-work} places Cudele in the context of other related
work.

