\begin{abstract}

HPC and data center scale application developers are abandoning POSIX IO
because the file system metadata synchronization and serialization overheads of
providing strong consistency and durability are too costly -- and often
unnecessary -- for their applications.  Unfortunately, designing file systems
with weaker consistency or durability excludes applications that rely on
stronger guarantees, forcing developers to re-write their applications or
deploy them on a different system.  Users can mount multiple systems in the
global namespace but this means (1) provisioning separate storage clusters and
(2) manually moving data across system boundaries.  We present a framework and
API that lets clients specify their consistency/durability requirements and
dynamically assign them to subtrees in the same namespace, allowing users to
optimize subtrees over time and space for different workloads.  We confirm the
performance benefits of techniques presented in related work but also explore
new consistency/durability metadata designs, all integrated over the same
storage system.  By custom fitting a subtree to a create-heavy application, we
show similar speedups to related work \newcomment{(up to \(91.7\times\)
speedup) but more importantly, our prototype can custom fit subtrees in the
same namespace to applications common in large data centers, such as
checkpoint-restart (\(91.7\times\), user home directories (within a 0.03
standard deviation from optimal), and clients checking for partial results
(only a 2\% overhead).}  \oldcomment{ and can scale to 2\(\times\) as many
clients when compared to our baseline system.}

\end{abstract}
