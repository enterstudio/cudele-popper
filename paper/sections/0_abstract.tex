\begin{abstract}

HPC and data center scale application developers are abandoning POSIX IO
because the file system metadata synchronization and serialization overheads of
providing strong consistency and durability are too costly -- and often
unnecessary -- for their applications.  Unfortunately, designing file systems
with weaker consistency or durability excludes applications that rely on
stronger guarantees, forcing developers to re-write their applications or
deploy them on a different system.  Users can mount multiple systems in the
global namespace but this means (1) provisioning separate storage clusters and
(2) manually moving data across system boundaries.  We present a framework and
API that lets \oldcomment{clients}\newcomment{administrators} specify their
consistency/durability requirements and dynamically assign them to subtrees in
the same namespace, allowing administrators to optimize subtrees over time and
space for different workloads.  We confirm the performance benefits of
techniques presented in related work but also explore new
consistency/durability metadata designs, all integrated over the same storage
system.  By custom fitting a subtree to a create-heavy application, we show
similar speedups to related work \newcomment{but more importantly, our
prototype can custom fit subtrees in the same namespace to applications common
in large data centers, such as checkpoint-restart (91.7\(\times\), user home
directories (within a 0.03 standard deviation from optimal), and
\oldcomment{clients}\newcomment{users} checking for partial results (only a 2\%
overhead).}  \oldcomment{ and can scale to 2\(\times\) as many clients when
compared to our baseline system.}

\end{abstract}
